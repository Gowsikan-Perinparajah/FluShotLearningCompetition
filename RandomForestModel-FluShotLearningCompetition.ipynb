{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd363bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import re\n",
    "\n",
    "#data import libraries\n",
    "import pandas as pd\n",
    "\n",
    "#path libraries\n",
    "from pathlib import Path\n",
    "import os.path as osp\n",
    "\n",
    "#math libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "#plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "#sklearn libraries for data cleaning\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler #normalize your dataset\n",
    "from sklearn.model_selection import train_test_split #split data to train and test data\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler #normalize your dataset\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "#sklearn library for linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#sklearn libraries for regularized regression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "#sklearn library for logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "#sklearn library for knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#sklearn libraries for decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "#sklearn libraries for data cleaning and cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler #normalize your dataset\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "#sklearn library for naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#sklearn library for support vector machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#sklearn library for neural networks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#optuna\n",
    "import optuna\n",
    "\n",
    "#sklearn library for scores and errors\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score, log_loss\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "#feature selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "\n",
    "#imbalanced smotes\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#warnings \n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d1d5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "error",
     "timestamp": 1688240535473,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "66c06d82",
    "outputId": "b9738d24-41a8-447b-c807-3e8bbd0fec61"
   },
   "outputs": [],
   "source": [
    "vaccine_features = pd.read_csv(\"training_set_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87ffe8",
   "metadata": {
    "executionInfo": {
     "elapsed": 21490,
     "status": "aborted",
     "timestamp": 1688240464360,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "ce353581"
   },
   "outputs": [],
   "source": [
    "vaccine_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223343a",
   "metadata": {
    "executionInfo": {
     "elapsed": 21486,
     "status": "aborted",
     "timestamp": 1688240464361,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "54f68dec"
   },
   "outputs": [],
   "source": [
    "vaccine_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212d571",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1688240464472,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "2728a10e"
   },
   "outputs": [],
   "source": [
    "vaccine_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1613758",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1688240464473,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "e97006fa"
   },
   "outputs": [],
   "source": [
    "vaccine_labels = pd.read_csv(\"training_set_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c8c27",
   "metadata": {
    "executionInfo": {
     "elapsed": 21582,
     "status": "aborted",
     "timestamp": 1688240464473,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "2444869d"
   },
   "outputs": [],
   "source": [
    "vaccine_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806532d",
   "metadata": {
    "executionInfo": {
     "elapsed": 21576,
     "status": "aborted",
     "timestamp": 1688240464473,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "f0b1ada2"
   },
   "outputs": [],
   "source": [
    "vaccine_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbf163",
   "metadata": {
    "executionInfo": {
     "elapsed": 21572,
     "status": "aborted",
     "timestamp": 1688240464474,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "270bae60"
   },
   "outputs": [],
   "source": [
    "vaccine_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b6ee3",
   "metadata": {
    "executionInfo": {
     "elapsed": 21571,
     "status": "aborted",
     "timestamp": 1688240464474,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "c8004815"
   },
   "outputs": [],
   "source": [
    "vaccine = pd.merge(vaccine_features, vaccine_labels, on=\"respondent_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72e9a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 21568,
     "status": "aborted",
     "timestamp": 1688240464474,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "58067c53"
   },
   "outputs": [],
   "source": [
    "vaccine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5f0e0",
   "metadata": {
    "executionInfo": {
     "elapsed": 21563,
     "status": "aborted",
     "timestamp": 1688240464474,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "457f0c40"
   },
   "outputs": [],
   "source": [
    "vaccine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc15bc",
   "metadata": {
    "executionInfo": {
     "elapsed": 21558,
     "status": "aborted",
     "timestamp": 1688240464474,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "1de2e372"
   },
   "outputs": [],
   "source": [
    "vaccine.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c8774",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1688240464589,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "1afd2ae5"
   },
   "outputs": [],
   "source": [
    "# this is the function that plots the custom confusion matrix with the colors and labels \n",
    "def confusion_matrix_plotting(cm, title):\n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(np.eye(2), annot=cm, fmt='g', annot_kws={'size': 25},\n",
    "            cmap=sns.color_palette(['tomato', 'palegreen']), cbar=False,\n",
    "            yticklabels=['Vaccinated', 'Not Vaccinated'], xticklabels=['Vaccinated', 'Not Vaccinated'], ax=ax)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.tick_params(labelsize=14, length=0)\n",
    "\n",
    "    ax.set_title(title, size=18, pad=10)\n",
    "    ax.set_xlabel('Predicted', size=14)\n",
    "    ax.set_ylabel('Actual', size=14)\n",
    "\n",
    "    additional_texts = ['(True Positive)', '(False Negative)', '(False Positive)', '(True Negative)']\n",
    "    for text_elt, additional_text in zip(ax.texts, additional_texts):\n",
    "        ax.text(*text_elt.get_position(), '\\n' + additional_text, color=text_elt.get_color(),\n",
    "            ha='center', va='top', size=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fdbc72",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1688240464589,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "829a2e12"
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(test, prediction):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    fpr1, tpr1, threshold1 = roc_curve(test, prediction)\n",
    "    roc_auc = metrics.auc(fpr1, tpr1)\n",
    "    display = RocCurveDisplay(fpr=fpr1, tpr=tpr1, roc_auc=roc_auc,estimator_name='example estimator')\n",
    "    display.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def quick_evaluate_with_dt(X_train, X_test, y_train, y_test, name, balance_weights=False):\n",
    "    \n",
    "    cw = None\n",
    "    if balance_weights == True:\n",
    "        cw = 'balanced'\n",
    "        \n",
    "    clf = DecisionTreeClassifier(random_state=0, class_weight=cw)\n",
    "    clf = RandomForestClassifier(random_state=0, n_estimators=100, class_weight=cw)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    accuracy       = accuracy_score(y_test, y_pred)\n",
    "    f1             = f1_score(y_test, y_pred)\n",
    "    recall         = tn/(tn+fp)\n",
    "    precision      = precision_score(y_test, y_pred)\n",
    "    roc_auc        = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    df = pd.DataFrame({\"Method\"    : [name],\n",
    "                       \"Neg\"       : [tn + fn],\n",
    "                       \"True Neg\"  : [tn],\n",
    "                       \"False Neg\" : [fn],\n",
    "                       \"Pos\"       : [tp + fp],\n",
    "                       \"TP\"        : [tp],\n",
    "                       \"FP\"        : [fp],\n",
    "                       \"Accuracy\"  : [accuracy],\n",
    "                       \"Recall\"    : [recall],\n",
    "                       \"Precision\" : [precision],\n",
    "                       \"F1\"        : [f1],\n",
    "                       \"AUC\"       : [roc_auc],\n",
    "                      })\n",
    "    \n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161ce70",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1688240464589,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "c7f55c5b"
   },
   "outputs": [],
   "source": [
    "X_seasonal = vaccine.drop(columns=['h1n1_vaccine', 'seasonal_vaccine', 'respondent_id'], axis=1)\n",
    "y_seasonal = vaccine['seasonal_vaccine']\n",
    "X_seasonal_train, X_seasonal_test, y_seasonal_train, y_seasonal_test = train_test_split(X_seasonal, y_seasonal, test_size=0.2, stratify=y_seasonal, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632978e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_h1n1 = vaccine.drop(columns=['h1n1_vaccine', 'seasonal_vaccine', 'respondent_id'], axis=1)\n",
    "y_h1n1 = vaccine['h1n1_vaccine']\n",
    "X_h1n1_train, X_h1n1_test, y_h1n1_train, y_h1n1_test = train_test_split(X_h1n1, y_h1n1, test_size=0.2, stratify=y_h1n1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of numeric features \n",
    "numeric_features = ['h1n1_concern', 'h1n1_knowledge',  'behavioral_face_mask',\n",
    "            'behavioral_wash_hands', 'behavioral_large_gatherings',\n",
    "                     'behavioral_touch_face',\n",
    "            'doctor_recc_h1n1', 'chronic_med_condition',\n",
    "                    'child_under_6_months', 'health_worker',\n",
    "            'health_insurance', 'opinion_h1n1_vacc_effective',\n",
    "                    'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc','doctor_recc_seasonal',\n",
    "                 'opinion_seas_vacc_effective','opinion_seas_risk', 'opinion_seas_sick_from_vacc', 'household_adults',\n",
    "             'household_children', 'behavioral_outside_home', 'behavioral_antiviral_meds',\n",
    "                    'behavioral_avoidance'\n",
    "                    ]\n",
    "\n",
    "# list of categorical features\n",
    "categorical_features = ['hhs_geo_region', 'census_msa', 'employment_industry', 'employment_occupation', \n",
    "                        'marital_status', 'race', 'sex', 'rent_or_own', 'age_group', \n",
    "                        'employment_status', 'education', 'income_poverty']\n",
    "\n",
    "\n",
    "# here we do the data cleaning for the numerical features, fill in missing values using the mean and the scaling the data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "# here we do the data cleaning for the categorical features, fill in missing values using the most frequent \n",
    "# then use one hot encoder to create dummy variables and just ignore unknown variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder())])\n",
    "\n",
    "\n",
    "# here we use column transformer to do all the numerical and categorical feature data cleaning in one function\n",
    "preprocessor4 = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)],\n",
    "            remainder = 'passthrough',\n",
    "            sparse_threshold=0)\n",
    "\n",
    "a = X_seasonal_train\n",
    "\n",
    "# transform the train by using the data cleaning stuff above to fill in missing data and create dummy variables\n",
    "processed_data = preprocessor4.fit_transform(a)\n",
    "\n",
    "# Get the feature names for the transformed data\n",
    "feature_names = preprocessor4.named_transformers_['cat']\\\n",
    "                    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numeric feature names with categorical feature names\n",
    "all_feature_names = numeric_features + list(feature_names)\n",
    "\n",
    "# Convert the processed data array back into a DataFrame\n",
    "X_seasonal_train = pd.DataFrame(processed_data, columns=all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seasonal_train[\"seasonal_vaccine_effectiveness\"] = X_seasonal_train[\"opinion_seas_vacc_effective\"]*X_seasonal_train[\"opinion_seas_sick_from_vacc\"]\n",
    "X_seasonal_train[\"seasonal_household\"]=X_seasonal_train[\"household_children\"]*X_seasonal_train[\"household_adults\"]\n",
    "\n",
    "X_seasonal_train = X_seasonal_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "X_seasonal_train = X_seasonal_train.loc[:,~X_seasonal_train.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seasonal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d896b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of probability of studies being late\n",
    "y_seasonal_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of numeric features \n",
    "numeric_features = ['h1n1_concern', 'h1n1_knowledge',  'behavioral_face_mask',\n",
    "            'behavioral_wash_hands', 'behavioral_large_gatherings',\n",
    "                     'behavioral_touch_face',\n",
    "            'doctor_recc_h1n1', 'chronic_med_condition',\n",
    "                    'child_under_6_months', 'health_worker',\n",
    "            'health_insurance', 'opinion_h1n1_vacc_effective',\n",
    "                    'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc','doctor_recc_seasonal',\n",
    "                 'opinion_seas_vacc_effective','opinion_seas_risk', 'opinion_seas_sick_from_vacc', 'household_adults',\n",
    "             'household_children', 'behavioral_outside_home', 'behavioral_antiviral_meds',\n",
    "                    'behavioral_avoidance'\n",
    "                    ]\n",
    "\n",
    "# list of categorical features\n",
    "categorical_features = ['hhs_geo_region', 'census_msa', 'employment_industry', 'employment_occupation', \n",
    "                        'marital_status', 'race', 'sex', 'rent_or_own', 'age_group', \n",
    "                        'employment_status', 'education', 'income_poverty']\n",
    "\n",
    "\n",
    "# here we do the data cleaning for the numerical features, fill in missing values using the mean and the scaling the data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "# here we do the data cleaning for the categorical features, fill in missing values using the most frequent \n",
    "# then use one hot encoder to create dummy variables and just ignore unknown variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder())])\n",
    "\n",
    "\n",
    "# here we use column transformer to do all the numerical and categorical feature data cleaning in one function\n",
    "preprocessor4 = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)],\n",
    "            remainder = 'passthrough',\n",
    "            sparse_threshold=0)\n",
    "\n",
    "a = X_h1n1_train\n",
    "\n",
    "# transform the train by using the data cleaning stuff above to fill in missing data and create dummy variables\n",
    "processed_data = preprocessor4.fit_transform(a)\n",
    "\n",
    "# Get the feature names for the transformed data\n",
    "feature_names = preprocessor4.named_transformers_['cat']\\\n",
    "                    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numeric feature names with categorical feature names\n",
    "all_feature_names = numeric_features + list(feature_names)\n",
    "\n",
    "# Convert the processed data array back into a DataFrame\n",
    "X_h1n1_train = pd.DataFrame(processed_data, columns=all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_h1n1_train[\"h1n1_flu\"] = X_h1n1_train[\"h1n1_concern\"]*X_h1n1_train[\"h1n1_knowledge\"]\n",
    "X_h1n1_train[\"h1n1_vaccine_effectiveness\"] = X_h1n1_train[\"opinion_h1n1_vacc_effective\"]*X_h1n1_train[\"opinion_h1n1_sick_from_vacc\"]\n",
    "X_h1n1_train[\"h1n1_chronic\"] = X_h1n1_train[\"chronic_med_condition\"]*X_h1n1_train[\"doctor_recc_h1n1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c84891",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_h1n1_train = X_h1n1_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "X_h1n1_train = X_h1n1_train.loc[:,~X_h1n1_train.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d992e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_h1n1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h1n1_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a3a34",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1688240464589,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "e4bf9e1d"
   },
   "outputs": [],
   "source": [
    "#LGBM Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40273fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, F, t):\n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=25), # this is the number of trees that the model will build for training\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 10, 100, step=5), # the minimum number of samples (or records) that need to be in a node/box before it can split to new ones \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3,15), # how deep do you want the tree to be \n",
    "        \"max_features\": trial.suggest_float(\"max_features\", 0.05, 1.0, step=0.01), # how many features or columns do you want to use to build a tree, this helps with generalizing your model as the less features you give, it will help with predicting new data\n",
    "        \"max_samples\": trial.suggest_float(\"max_samples\", 0.05, 1.0, step=0.01), # how many samples or records do you want to use to build a tree, this helps with generalizing your model as the less features you give, it will help with predicting new data\n",
    "        \"class_weight\":  trial.suggest_categorical(\"class_weight\", [\"balanced\", None]), # using balanced data set or original dataset\n",
    "    }\n",
    "\n",
    " \n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10) # split the data into 10 equally weighted folds\n",
    "    cv_scores = np.empty(10) # create an empty array of 10 elements to take an average of each trial \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(F, t)): \n",
    "        X_train_f, X_test_f = F.iloc[train_idx], F.iloc[test_idx] # assigning the train part for the fold\n",
    "        y_train_f, y_test_f = t[train_idx], t[test_idx] # assigning the test part for the fold\n",
    "\n",
    " \n",
    "\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=-1 , **param_grid, verbose=0) #here we pass in the model, could be any ML algorithm and verbose is the parameter used to print each trial\n",
    "        \n",
    "        model.fit(X_train_f, y_train_f) # fit the data\n",
    "\n",
    "        preds = model.predict_proba(X_test_f)[:,1] #get the predicted probability to calculate the AUC score\n",
    "\n",
    "        cv_scores[idx] = roc_auc_score(y_test_f, preds) # calculate the auc score for each fold\n",
    "\n",
    " \n",
    "\n",
    "    return np.mean(cv_scores) # take the average AUC score after running 10 folds \n",
    "\n",
    "# this code is for one trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seasonal_train.reset_index(drop=True, inplace=True)\n",
    "y_seasonal_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "study_seasonal = optuna.create_study(direction='maximize', study_name=\"Random Forest Classifier\")\n",
    "func = lambda trial: objective(trial, X_seasonal_train, y_seasonal_train)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_seasonal.optimize(func, n_trials=100, n_jobs=-1, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters:', study_seasonal.best_params)\n",
    "print('Best Score:', study_seasonal.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151222f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_h1n1_train.reset_index(drop=True, inplace=True)\n",
    "y_h1n1_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "study_h1n1 = optuna.create_study(direction='maximize', study_name=\"Random Forest Classifier\")\n",
    "func = lambda trial: objective(trial, X_h1n1_train, y_h1n1_train)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_h1n1.optimize(func, n_trials=100, n_jobs=-1, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ca7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters:', study_h1n1.best_params)\n",
    "print('Best Score:', study_h1n1.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2667f7",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1688240464590,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "63598fde"
   },
   "outputs": [],
   "source": [
    "clf_seasonal = RandomForestClassifier(**study_seasonal.best_params, n_jobs=-1, random_state=42, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd494be",
   "metadata": {
    "executionInfo": {
     "elapsed": 21653,
     "status": "aborted",
     "timestamp": 1688240464590,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "258f80bc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have your data 'X' (input features) and 'y' (target labels)\n",
    "\n",
    "# Define the number of folds (K)\n",
    "k = 10\n",
    "\n",
    "# Initialize lists to store the evaluation metrics\n",
    "confusion_matrices = []\n",
    "accuracy_scores = []\n",
    "truepositive = []\n",
    "truenegative = []\n",
    "falsepositive = []\n",
    "falsenegative = []\n",
    "specificity = []\n",
    "sensitivity = []\n",
    "LogLoss = []\n",
    "auc = []\n",
    "test = []\n",
    "pred_proba = []\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "\n",
    "X1 = X_seasonal_train.values\n",
    "y1 = y_seasonal_train.values\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X1):\n",
    "    # Split the data into training and test sets\n",
    "    X_train_lr, X_test_lr = X1[train_index], X1[test_index]\n",
    "    y_train_lr, y_test_lr = y1[train_index], y1[test_index]\n",
    "\n",
    "    # Train your model on the training set\n",
    "    clf_seasonal.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf_seasonal.predict(X_test_lr)\n",
    "    y_pred_proba = clf_seasonal.predict_proba(X_test_lr)\n",
    "\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_test_lr, y_pred)\n",
    "    confusion_matrices.append(np.flip(cm))\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    truenegative.append(tn)\n",
    "    falsepositive.append(fp)\n",
    "    falsenegative.append(fn)\n",
    "    truepositive.append(tp)\n",
    "\n",
    "\n",
    "    #Compute Specificity and Sensitivity\n",
    "    Sensitivity = np.round(tp / (tp+fn),4)\n",
    "    sensitivity.append(Sensitivity)\n",
    "    Specificity = np.round(tn / (tn+fp),4)\n",
    "    specificity.append(Specificity)\n",
    "\n",
    "\n",
    "    #Compute the log loss\n",
    "    logloss = log_loss(y_test_lr, y_pred, labels=[0, 1])\n",
    "    LogLoss.append(logloss)\n",
    "\n",
    "    # Compute the classification report\n",
    "    accuracy = classification_report(y_test_lr, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    AUC = roc_auc_score(y_test_lr, y_pred_proba[:,1], average='macro')\n",
    "    print(AUC)\n",
    "    auc.append(AUC)\n",
    "\n",
    "    pred_proba.append(y_pred_proba[:,1])\n",
    "    test.append(y_test_lr)\n",
    "\n",
    "# Print the confusion matrices and accuracy scores for each fold\n",
    "for fold in range(k):\n",
    "    confusion_matrix_plotting(confusion_matrices[fold], (\"Random Forest \" + f\"Fold {fold+1}\"))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(\"\\n\")\n",
    "    print(accuracy_scores[fold])\n",
    "    print(\"\\n\")\n",
    "    print(\"Sensitivity: \" + str(sensitivity[fold]))\n",
    "    print(\"Specificity: \" + str(specificity[fold]))\n",
    "    print(\"Log Loss: \" + str(LogLoss[fold]))\n",
    "    print(\"\\n\")\n",
    "    plot_roc_curve(test[fold],pred_proba[fold])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8088f8",
   "metadata": {
    "executionInfo": {
     "elapsed": 21650,
     "status": "aborted",
     "timestamp": 1688240464590,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "e94fa708"
   },
   "outputs": [],
   "source": [
    "# Python program to get average of a list\n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5af831",
   "metadata": {
    "executionInfo": {
     "elapsed": 21649,
     "status": "aborted",
     "timestamp": 1688240464590,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "15a23046",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = np.array([[Average(truepositive), Average(falsenegative)], [Average(falsepositive), Average(truenegative)]]) \n",
    "confusion_matrix_plotting(test, (\"Random Forest Seasonal Average\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ce003",
   "metadata": {
    "executionInfo": {
     "elapsed": 21650,
     "status": "aborted",
     "timestamp": 1688240464591,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "3380e109"
   },
   "outputs": [],
   "source": [
    "print(Average(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_h1n1 = RandomForestClassifier(**study_h1n1.best_params, n_jobs=-1, random_state=42, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9711e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have your data 'X' (input features) and 'y' (target labels)\n",
    "\n",
    "# Define the number of folds (K)\n",
    "k = 10\n",
    "\n",
    "# Initialize lists to store the evaluation metrics\n",
    "confusion_matrices = []\n",
    "accuracy_scores = []\n",
    "truepositive = []\n",
    "truenegative = []\n",
    "falsepositive = []\n",
    "falsenegative = []\n",
    "specificity = []\n",
    "sensitivity = []\n",
    "LogLoss = []\n",
    "auc = []\n",
    "test = []\n",
    "pred_proba = []\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "\n",
    "X1 = X_h1n1_train.values\n",
    "y1 = y_h1n1_train.values\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X1):\n",
    "    # Split the data into training and test sets\n",
    "    X_train_lr, X_test_lr = X1[train_index], X1[test_index]\n",
    "    y_train_lr, y_test_lr = y1[train_index], y1[test_index]\n",
    "\n",
    "    # Train your model on the training set\n",
    "    clf_h1n1.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf_h1n1.predict(X_test_lr)\n",
    "    y_pred_proba = clf_h1n1.predict_proba(X_test_lr)\n",
    "\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_test_lr, y_pred)\n",
    "    confusion_matrices.append(np.flip(cm))\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    truenegative.append(tn)\n",
    "    falsepositive.append(fp)\n",
    "    falsenegative.append(fn)\n",
    "    truepositive.append(tp)\n",
    "\n",
    "\n",
    "    #Compute Specificity and Sensitivity\n",
    "    Sensitivity = np.round(tp / (tp+fn),4)\n",
    "    sensitivity.append(Sensitivity)\n",
    "    Specificity = np.round(tn / (tn+fp),4)\n",
    "    specificity.append(Specificity)\n",
    "\n",
    "\n",
    "    #Compute the log loss\n",
    "    logloss = log_loss(y_test_lr, y_pred, labels=[0, 1])\n",
    "    LogLoss.append(logloss)\n",
    "\n",
    "    # Compute the classification report\n",
    "    accuracy = classification_report(y_test_lr, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    AUC = roc_auc_score(y_test_lr, y_pred_proba[:,1], average='macro')\n",
    "    print(AUC)\n",
    "    auc.append(AUC)\n",
    "\n",
    "    pred_proba.append(y_pred_proba[:,1])\n",
    "    test.append(y_test_lr)\n",
    "\n",
    "# Print the confusion matrices and accuracy scores for each fold\n",
    "for fold in range(k):\n",
    "    confusion_matrix_plotting(confusion_matrices[fold], (\"Random Forest \" + f\"Fold {fold+1}\"))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(\"\\n\")\n",
    "    print(accuracy_scores[fold])\n",
    "    print(\"\\n\")\n",
    "    print(\"Sensitivity: \" + str(sensitivity[fold]))\n",
    "    print(\"Specificity: \" + str(specificity[fold]))\n",
    "    print(\"Log Loss: \" + str(LogLoss[fold]))\n",
    "    print(\"\\n\")\n",
    "    plot_roc_curve(test[fold],pred_proba[fold])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_h1n1 = np.array([[Average(truepositive), Average(falsenegative)], [Average(falsepositive), Average(truenegative)]]) \n",
    "confusion_matrix_plotting(test_h1n1, (\"Random Forest H1N1 Average\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Average(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_seasonal_test\n",
    "\n",
    "# transform the train by using the data cleaning stuff above to fill in missing data and create dummy variables\n",
    "processed_data = preprocessor4.transform(a)\n",
    "\n",
    "# Get the feature names for the transformed data\n",
    "feature_names = preprocessor4.named_transformers_['cat']\\\n",
    "                    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numeric feature names with categorical feature names\n",
    "all_feature_names = numeric_features + list(feature_names)\n",
    "\n",
    "# Convert the processed data array back into a DataFrame\n",
    "X_seasonal_test = pd.DataFrame(processed_data, columns=all_feature_names)\n",
    "\n",
    "X_seasonal_test[\"seasonal_vaccine_effectiveness\"] = X_seasonal_test[\"opinion_seas_vacc_effective\"]*X_seasonal_test[\"opinion_seas_sick_from_vacc\"]\n",
    "X_seasonal_test[\"seasonal_household\"]=X_seasonal_test[\"household_children\"]*X_seasonal_test[\"household_adults\"]\n",
    "\n",
    "X_seasonal_test = X_seasonal_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "X_seasonal_test = X_seasonal_test.loc[:,~X_seasonal_test.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036544f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_seasonal.fit(X_seasonal_train, y_seasonal_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb82290",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = (clf_seasonal.feature_importances_)/sum(clf_seasonal.feature_importances_)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bcdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[-10:]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.style.use('dark_background')\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"cyan\", height=0.2, align='center')\n",
    "plt.yticks(range(len(indices)), [X_seasonal_train.columns.values.tolist()[i] for i in indices], fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel(\"Feature Importance\", fontsize=18)\n",
    "plt.ylabel(\"Features\", fontsize=18)\n",
    "plt.title(\"Feature Importance for Random Forest - Seasonal\", fontsize=18)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661419d",
   "metadata": {
    "executionInfo": {
     "elapsed": 21649,
     "status": "aborted",
     "timestamp": 1688240464591,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "a50e5fb2"
   },
   "outputs": [],
   "source": [
    "y_seasonal_pred = clf_seasonal.predict(X_seasonal_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_seasonal_test)\n",
    "\n",
    "false_predictions = np.where(y_seasonal_pred != y_seasonal_test)[0]\n",
    "\n",
    "# Print the indices of false predictions and their corresponding true and predicted labels\n",
    "for idx in false_predictions:\n",
    "    print(f\"Index: {idx}, True label: {y_seasonal_test.to_list()[idx]}, Predicted label: {y_seasonal_pred[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a23d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_seasonal_test_new = y_seasonal_test.to_list()\n",
    "data_seasonal = pd.concat([X_seasonal_test, pd.DataFrame(data={'Prediction': y_seasonal_pred, 'Actual': y_seasonal_test_new})], axis=1)\n",
    "data_seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seasonal.iloc[:,[-2,-1]].to_csv(\"seasonalrf.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negative\n",
    "data_seasonal.query('Prediction==0 & Actual==1 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positive\n",
    "data_seasonal.query('Prediction==1 & Actual==0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30206025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive\n",
    "data_seasonal.query('Prediction==1 & Actual==1 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfdf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Negative\n",
    "data_seasonal.query('Prediction==0 & Actual==0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd30a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 21648,
     "status": "aborted",
     "timestamp": 1688240464591,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "99ab73a4"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_seasonal_test, y_seasonal_pred)\n",
    "confusion_matrix_plotting(np.flip(cm), \"Random Forest Seasonal Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9332c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "\n",
    "fn = cm[1][0]\n",
    "tp = cm[1][1]\n",
    "\n",
    "negative = tn + fp\n",
    "positive = tp + fn\n",
    "\n",
    "total = tn + fp + tp + fn\n",
    "\n",
    "no_weights = []\n",
    "no_weights.append((fp/negative)*100)\n",
    "no_weights.append((fn/positive)*100)\n",
    "no_weights.append(((fn+fp)/total)*100)\n",
    "\n",
    "yes_weights = []\n",
    "yes_weights.append((tn/negative)*100)\n",
    "yes_weights.append((tp/positive)*100)\n",
    "yes_weights.append(((tn+tp)/total)*100)\n",
    "\n",
    "\n",
    "species = (\n",
    "    \"No\",\n",
    "    \"Yes\",\n",
    "    \"Total\",\n",
    ")\n",
    "weight_counts = {\n",
    "    \"Correct\": yes_weights,\n",
    "    \"Wrong\": no_weights,\n",
    "\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "bottom = np.zeros(3)\n",
    "\n",
    "j = 0\n",
    "\n",
    "colors = ['palegreen', 'tomato']\n",
    "\n",
    "for boolean, weight_count in weight_counts.items():\n",
    "    p = ax.bar(species, weight_count, label=boolean, bottom=bottom, color=colors[j])\n",
    "    \n",
    "    for i, rect in enumerate(p):\n",
    "        height = rect.get_height()\n",
    "        if height >= 0:\n",
    "            ax.text(rect.get_x() + rect.get_width() / 2, \n",
    "                    bottom[i] + height / 2, \n",
    "                    \"{:.1f}%\".format(height),\n",
    "                    ha='center', \n",
    "                    va='center',\n",
    "                   fontsize=12,\n",
    "                   weight=\"bold\")\n",
    "    \n",
    "    bottom += weight_count\n",
    "    j += 1\n",
    "\n",
    "ax.set_title(\"Total Proportion of Correct Predictions per Class - Seasonal\")\n",
    "ax.legend(bbox_to_anchor=(1.02, 1))\n",
    "ax.set_yticks(ticks=[0, 20, 40, 60, 80, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49741a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 21648,
     "status": "aborted",
     "timestamp": 1688240464591,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "02ca8a52"
   },
   "outputs": [],
   "source": [
    "sensitivity = np.round(tp / (tp+fn),4)\n",
    "print(\"Sensitivity: \" + str(sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6b840",
   "metadata": {
    "executionInfo": {
     "elapsed": 21646,
     "status": "aborted",
     "timestamp": 1688240464591,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "3b84cdb8"
   },
   "outputs": [],
   "source": [
    "specificity = np.round(tn / (tn+fp),4)\n",
    "print(\"Specificity: \" + str(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c218761",
   "metadata": {
    "executionInfo": {
     "elapsed": 21645,
     "status": "aborted",
     "timestamp": 1688240464591,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "e802b559"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_seasonal_test, y_seasonal_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61851b",
   "metadata": {
    "executionInfo": {
     "elapsed": 21644,
     "status": "aborted",
     "timestamp": 1688240464592,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "4e717fcc"
   },
   "outputs": [],
   "source": [
    "y_seasonal_pred_proba = clf_seasonal.predict_proba(X_seasonal_test)\n",
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(7,7))\n",
    "fpr1, tpr1, threshold1 = roc_curve(y_seasonal_test, y_seasonal_pred_proba[:,1])\n",
    "roc_auc = metrics.auc(fpr1, tpr1)\n",
    "print(roc_auc)\n",
    "display = RocCurveDisplay(fpr=fpr1, tpr=tpr1, roc_auc=np.round(roc_auc,4))\n",
    "display.plot(color=\"cyan\")\n",
    "plt.grid(False)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"False Positive Rate (FPR)\", fontsize=14)\n",
    "plt.ylabel(\"True Positive Rate (TPR)\", fontsize=14)\n",
    "plt.title(\"ROC-AUC Curve for Random Forest\", fontsize=18)\n",
    "plt.xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=14)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=14)\n",
    "legend_properties = {'size': 16, 'weight':'bold'}\n",
    "plt.legend(loc=4, labels=['Random Forest (AUC: ' + str(np.round(roc_auc,4)) + \")\"], prop=legend_properties, frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_h1n1_test\n",
    "\n",
    "# transform the train by using the data cleaning stuff above to fill in missing data and create dummy variables\n",
    "processed_data = preprocessor4.transform(a)\n",
    "\n",
    "# Get the feature names for the transformed data\n",
    "feature_names = preprocessor4.named_transformers_['cat']\\\n",
    "                    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numeric feature names with categorical feature names\n",
    "all_feature_names = numeric_features + list(feature_names)\n",
    "\n",
    "# Convert the processed data array back into a DataFrame\n",
    "X_h1n1_test = pd.DataFrame(processed_data, columns=all_feature_names)\n",
    "\n",
    "X_h1n1_test[\"h1n1_flu\"] = X_h1n1_test[\"h1n1_concern\"]*X_h1n1_test[\"h1n1_knowledge\"]\n",
    "X_h1n1_test[\"h1n1_vaccine_effectiveness\"] = X_h1n1_test[\"opinion_h1n1_vacc_effective\"]*X_h1n1_test[\"opinion_h1n1_sick_from_vacc\"]\n",
    "X_h1n1_test[\"h1n1_chronic\"] = X_h1n1_test[\"chronic_med_condition\"]*X_h1n1_test[\"doctor_recc_h1n1\"]\n",
    "\n",
    "X_h1n1_test = X_h1n1_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "X_h1n1_test = X_h1n1_test.loc[:,~X_h1n1_test.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_h1n1.fit(X_h1n1_train, y_h1n1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2794ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = (clf_h1n1.feature_importances_)/sum(clf_h1n1.feature_importances_)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[-10:]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.style.use('dark_background')\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"cyan\", height=0.2, align='center')\n",
    "plt.yticks(range(len(indices)), [X_h1n1_train.columns.values.tolist()[i] for i in indices], fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel(\"Feature Importance\", fontsize=18)\n",
    "plt.ylabel(\"Features\", fontsize=18)\n",
    "plt.title(\"Feature Importance for Random Forest - H1N1\", fontsize=18)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h1n1_pred = clf_h1n1.predict(X_h1n1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf34fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_h1n1_test)\n",
    "\n",
    "false_predictions = np.where(y_h1n1_pred != y_h1n1_test)[0]\n",
    "\n",
    "# Print the indices of false predictions and their corresponding true and predicted labels\n",
    "for idx in false_predictions:\n",
    "    print(f\"Index: {idx}, True label: {y_h1n1_test.to_list()[idx]}, Predicted label: {y_h1n1_pred[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h1n1_test_new = y_h1n1_test.to_list()\n",
    "data_h1n1 = pd.concat([X_h1n1_test, pd.DataFrame(data={'Prediction': y_h1n1_pred, 'Actual': y_h1n1_test_new})], axis=1)\n",
    "data_h1n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_h1n1.iloc[:,[-2,-1]].to_csv(\"h1n1rf.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negative\n",
    "data_h1n1.query('Prediction==0 & Actual==1 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positive\n",
    "data_h1n1.query('Prediction==1 & Actual==0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d00ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive\n",
    "data_h1n1.query('Prediction==1 & Actual==1 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Negative\n",
    "data_h1n1.query('Prediction==0 & Actual==0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980bdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_h1n1 = confusion_matrix(y_h1n1_test, y_h1n1_pred)\n",
    "confusion_matrix_plotting(np.flip(cm_h1n1), \"Random Forest H1N1 Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = cm_h1n1[0][0]\n",
    "fp = cm_h1n1[0][1]\n",
    "\n",
    "fn = cm_h1n1[1][0]\n",
    "tp = cm_h1n1[1][1]\n",
    "\n",
    "negative = tn + fp\n",
    "positive = tp + fn\n",
    "\n",
    "total = tn + fp + tp + fn\n",
    "\n",
    "no_weights = []\n",
    "no_weights.append((fp/negative)*100)\n",
    "no_weights.append((fn/positive)*100)\n",
    "no_weights.append(((fn+fp)/total)*100)\n",
    "\n",
    "yes_weights = []\n",
    "yes_weights.append((tn/negative)*100)\n",
    "yes_weights.append((tp/positive)*100)\n",
    "yes_weights.append(((tn+tp)/total)*100)\n",
    "\n",
    "\n",
    "species = (\n",
    "    \"No\",\n",
    "    \"Yes\",\n",
    "    \"Total\",\n",
    ")\n",
    "weight_counts = {\n",
    "    \"Correct\": yes_weights,\n",
    "    \"Wrong\": no_weights,\n",
    "\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "bottom = np.zeros(3)\n",
    "\n",
    "j = 0\n",
    "\n",
    "colors = ['palegreen', 'tomato']\n",
    "\n",
    "for boolean, weight_count in weight_counts.items():\n",
    "    p = ax.bar(species, weight_count, label=boolean, bottom=bottom, color=colors[j])\n",
    "    \n",
    "    for i, rect in enumerate(p):\n",
    "        height = rect.get_height()\n",
    "        if height >= 0:\n",
    "            ax.text(rect.get_x() + rect.get_width() / 2, \n",
    "                    bottom[i] + height / 2, \n",
    "                    \"{:.1f}%\".format(height),\n",
    "                    ha='center', \n",
    "                    va='center',\n",
    "                   fontsize=12,\n",
    "                   weight=\"bold\")\n",
    "    \n",
    "    bottom += weight_count\n",
    "    j += 1\n",
    "\n",
    "ax.set_title(\"Total Proportion of Correct Predictions per Class - H1N1\")\n",
    "ax.legend(bbox_to_anchor=(1.02, 1))\n",
    "ax.set_yticks(ticks=[0, 20, 40, 60, 80, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145cfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = np.round(tp / (tp+fn),4)\n",
    "print(\"Sensitivity: \" + str(sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = np.round(tn / (tn+fp),4)\n",
    "print(\"Specificity: \" + str(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb28433",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_h1n1_test, y_h1n1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8846693",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h1n1_pred_proba = clf_h1n1.predict_proba(X_h1n1_test)\n",
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(7,7))\n",
    "fpr1, tpr1, threshold1 = roc_curve(y_h1n1_test, y_h1n1_pred_proba[:,1])\n",
    "roc_auc = metrics.auc(fpr1, tpr1)\n",
    "print(roc_auc)\n",
    "display = RocCurveDisplay(fpr=fpr1, tpr=tpr1, roc_auc=np.round(roc_auc,4))\n",
    "display.plot(color=\"cyan\")\n",
    "plt.grid(False)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"False Positive Rate (FPR)\", fontsize=14)\n",
    "plt.ylabel(\"True Positive Rate (TPR)\", fontsize=14)\n",
    "plt.title(\"ROC-AUC Curve for LGBM\", fontsize=18)\n",
    "plt.xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=14)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=14)\n",
    "legend_properties = {'size': 16, 'weight':'bold'}\n",
    "plt.legend(loc=4, labels=['Random Forest (AUC: ' + str(np.round(roc_auc,4)) + \")\"], prop=legend_properties, frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of numeric features \n",
    "numeric_features = ['h1n1_concern', 'h1n1_knowledge',  'behavioral_face_mask',\n",
    "            'behavioral_wash_hands', 'behavioral_large_gatherings',\n",
    "                     'behavioral_touch_face',\n",
    "            'doctor_recc_h1n1', 'chronic_med_condition',\n",
    "                    'child_under_6_months', 'health_worker',\n",
    "            'health_insurance', 'opinion_h1n1_vacc_effective',\n",
    "                    'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc','doctor_recc_seasonal',\n",
    "                 'opinion_seas_vacc_effective','opinion_seas_risk', 'opinion_seas_sick_from_vacc', 'household_adults',\n",
    "             'household_children', 'behavioral_outside_home', 'behavioral_antiviral_meds',\n",
    "                    'behavioral_avoidance'\n",
    "                    ]\n",
    "\n",
    "# list of categorical features\n",
    "categorical_features = ['hhs_geo_region', 'census_msa', 'employment_industry', 'employment_occupation', \n",
    "                        'marital_status', 'race', 'sex', 'rent_or_own', 'age_group', \n",
    "                        'employment_status', 'education', 'income_poverty']\n",
    "\n",
    "\n",
    "# here we do the data cleaning for the numerical features, fill in missing values using the mean and the scaling the data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "# here we do the data cleaning for the categorical features, fill in missing values using the most frequent \n",
    "# then use one hot encoder to create dummy variables and just ignore unknown variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder())])\n",
    "\n",
    "\n",
    "# here we use column transformer to do all the numerical and categorical feature data cleaning in one function\n",
    "preprocessor4 = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)],\n",
    "            remainder = 'passthrough',\n",
    "            sparse_threshold=0)\n",
    "\n",
    "\n",
    "a = X_seasonal\n",
    "\n",
    "# transform the train by using the data cleaning stuff above to fill in missing data and create dummy variables\n",
    "processed_data = preprocessor4.fit_transform(a)\n",
    "\n",
    "# Get the feature names for the transformed data\n",
    "feature_names = preprocessor4.named_transformers_['cat']\\\n",
    "                    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numeric feature names with categorical feature names\n",
    "all_feature_names = numeric_features + list(feature_names)\n",
    "\n",
    "# Convert the processed data array back into a DataFrame\n",
    "X_seasonal = pd.DataFrame(processed_data, columns=all_feature_names)\n",
    "\n",
    "X_seasonal[\"seasonal_vaccine_effectiveness\"] = X_seasonal[\"opinion_seas_vacc_effective\"]*X_seasonal[\"opinion_seas_sick_from_vacc\"]\n",
    "X_seasonal[\"seasonal_household\"]=X_seasonal[\"household_children\"]*X_seasonal[\"household_adults\"]\n",
    "\n",
    "X_seasonal = X_seasonal.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "X_seasonal = X_seasonal.loc[:,~X_seasonal.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3298888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of numeric features \n",
    "numeric_features = ['h1n1_concern', 'h1n1_knowledge',  'behavioral_face_mask',\n",
    "            'behavioral_wash_hands', 'behavioral_large_gatherings',\n",
    "                     'behavioral_touch_face',\n",
    "            'doctor_recc_h1n1', 'chronic_med_condition',\n",
    "                    'child_under_6_months', 'health_worker',\n",
    "            'health_insurance', 'opinion_h1n1_vacc_effective',\n",
    "                    'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc','doctor_recc_seasonal',\n",
    "                 'opinion_seas_vacc_effective','opinion_seas_risk', 'opinion_seas_sick_from_vacc', 'household_adults',\n",
    "             'household_children', 'behavioral_outside_home', 'behavioral_antiviral_meds',\n",
    "                    'behavioral_avoidance'\n",
    "                    ]\n",
    "\n",
    "# list of categorical features\n",
    "categorical_features = ['hhs_geo_region', 'census_msa', 'employment_industry', 'employment_occupation', \n",
    "                        'marital_status', 'race', 'sex', 'rent_or_own', 'age_group', \n",
    "                        'employment_status', 'education', 'income_poverty']\n",
    "\n",
    "\n",
    "# here we do the data cleaning for the numerical features, fill in missing values using the mean and the scaling the data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "# here we do the data cleaning for the categorical features, fill in missing values using the most frequent \n",
    "# then use one hot encoder to create dummy variables and just ignore unknown variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder())])\n",
    "\n",
    "\n",
    "# here we use column transformer to do all the numerical and categorical feature data cleaning in one function\n",
    "preprocessor4 = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)],\n",
    "            remainder = 'passthrough',\n",
    "            sparse_threshold=0)\n",
    "\n",
    "\n",
    "a = X_h1n1\n",
    "\n",
    "# transform the train by using the data cleaning stuff above to fill in missing data and create dummy variables\n",
    "processed_data = preprocessor4.fit_transform(a)\n",
    "\n",
    "# Get the feature names for the transformed data\n",
    "feature_names = preprocessor4.named_transformers_['cat']\\\n",
    "                    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numeric feature names with categorical feature names\n",
    "all_feature_names = numeric_features + list(feature_names)\n",
    "\n",
    "# Convert the processed data array back into a DataFrame\n",
    "X_h1n1 = pd.DataFrame(processed_data, columns=all_feature_names)\n",
    "\n",
    "X_h1n1[\"h1n1_flu\"] = X_h1n1[\"h1n1_concern\"]*X_h1n1[\"h1n1_knowledge\"]\n",
    "X_h1n1[\"h1n1_vaccine_effectiveness\"] = X_h1n1[\"opinion_h1n1_vacc_effective\"]*X_h1n1[\"opinion_h1n1_sick_from_vacc\"]\n",
    "X_h1n1[\"h1n1_chronic\"] = X_h1n1[\"chronic_med_condition\"]*X_h1n1[\"doctor_recc_h1n1\"]\n",
    "\n",
    "X_h1n1 = X_h1n1.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "X_h1n1 = X_h1n1.loc[:,~X_h1n1.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3098d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_seasonal.fit(X_seasonal, y_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480412ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_seasonal.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507847c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_h1n1.fit(X_h1n1, y_h1n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e093c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_h1n1.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b505848",
   "metadata": {
    "executionInfo": {
     "elapsed": 21642,
     "status": "aborted",
     "timestamp": 1688240464592,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "fa473314"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_set_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64fe5c",
   "metadata": {
    "executionInfo": {
     "elapsed": 21641,
     "status": "aborted",
     "timestamp": 1688240464592,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "dd2f6d82"
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f459a",
   "metadata": {
    "executionInfo": {
     "elapsed": 21640,
     "status": "aborted",
     "timestamp": 1688240464592,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "52c84644"
   },
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf881ab",
   "metadata": {
    "executionInfo": {
     "elapsed": 21641,
     "status": "aborted",
     "timestamp": 1688240464593,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "3fb63909"
   },
   "outputs": [],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b76d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here apply the same data cleaning process to the test data set \n",
    "a = test.drop(columns=['respondent_id'], axis=1)\n",
    "\n",
    "processed_data = preprocessor4.transform(a)\n",
    "\n",
    "# Get the feature names for the transformed data\n",
    "feature_names = preprocessor4.named_transformers_['cat']\\\n",
    "                    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numeric feature names with categorical feature names\n",
    "all_feature_names = numeric_features + list(feature_names)\n",
    "\n",
    "# Convert the processed data array back into a DataFrame\n",
    "processed_df  = pd.DataFrame(processed_data, columns=all_feature_names)\n",
    "\n",
    "processed_df[\"seasonal_vaccine_effectiveness\"] = processed_df[\"opinion_seas_vacc_effective\"]*processed_df[\"opinion_seas_sick_from_vacc\"]\n",
    "processed_df[\"seasonal_household\"]=processed_df[\"household_children\"]*processed_df[\"household_adults\"]\n",
    "\n",
    "processed_df = processed_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "processed_df = processed_df.loc[:,~processed_df.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cf0e5",
   "metadata": {
    "executionInfo": {
     "elapsed": 21639,
     "status": "aborted",
     "timestamp": 1688240464594,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "14c39679"
   },
   "outputs": [],
   "source": [
    "prediction = clf_seasonal.predict_proba(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here apply the same data cleaning process to the test data set \n",
    "a = test.drop(columns=['respondent_id'], axis=1)\n",
    "\n",
    "processed_data = preprocessor4.transform(a)\n",
    "\n",
    "# Get the feature names for the transformed data\n",
    "feature_names = preprocessor4.named_transformers_['cat']\\\n",
    "                    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numeric feature names with categorical feature names\n",
    "all_feature_names = numeric_features + list(feature_names)\n",
    "\n",
    "# Convert the processed data array back into a DataFrame\n",
    "processed_df  = pd.DataFrame(processed_data, columns=all_feature_names)\n",
    "\n",
    "processed_df[\"h1n1_flu\"] = processed_df[\"h1n1_concern\"]*processed_df[\"h1n1_knowledge\"]\n",
    "processed_df[\"h1n1_vaccine_effectiveness\"] = processed_df[\"opinion_h1n1_vacc_effective\"]*processed_df[\"opinion_h1n1_sick_from_vacc\"]\n",
    "processed_df[\"h1n1_chronic\"] = processed_df[\"chronic_med_condition\"]*processed_df[\"doctor_recc_h1n1\"]\n",
    "\n",
    "processed_df = processed_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "processed_df = processed_df.loc[:,~processed_df.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_h1n1 = clf_h1n1.predict_proba(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa00d1",
   "metadata": {
    "executionInfo": {
     "elapsed": 21766,
     "status": "aborted",
     "timestamp": 1688240464722,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "c1a08900"
   },
   "outputs": [],
   "source": [
    "d = {'respondent_id': test['respondent_id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9488e0b",
   "metadata": {
    "executionInfo": {
     "elapsed": 21765,
     "status": "aborted",
     "timestamp": 1688240464722,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "da9f0dd3"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['h1n1_vaccine'] = prediction_h1n1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87acb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seasonal_vaccine'] = prediction[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3857b50",
   "metadata": {
    "executionInfo": {
     "elapsed": 21766,
     "status": "aborted",
     "timestamp": 1688240464724,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "d69f4bf8"
   },
   "outputs": [],
   "source": [
    "df.to_csv('rf.csv', sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bff0e",
   "metadata": {
    "executionInfo": {
     "elapsed": 21765,
     "status": "aborted",
     "timestamp": 1688240464724,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "03815551"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98b216",
   "metadata": {
    "executionInfo": {
     "elapsed": 21764,
     "status": "aborted",
     "timestamp": 1688240464724,
     "user": {
      "displayName": "Gowsikan Perinparajah",
      "userId": "18265124575842527721"
     },
     "user_tz": 240
    },
    "id": "4b11f7fd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1iZWroQlrV76TDP6zLTAyHJvsWbssa9J9",
     "timestamp": 1688063123224
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
